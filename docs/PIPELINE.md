# Pipeline

We call **the pipeline** all the steps that chat data goes through before reaching the report UI. This document describes each step so you can get a general idea of how it works.

The following diagram gives an overview of the pipeline:

<img src="./media/pipeline.svg">

Quick jump to sections:

1. [Input files](#1-input-files)
2. [Parsing](#2-parsing)
3. [Processing & Analysis](#3-processing--analysis)
4. [Compression and Encoding](#4-compression-and-encoding)
5. [Aggregate / Blocks](#5-aggregate--blocks)
6. [About message serialization](#about-message-serialization)

## 1. Input files

The input files are chat exports; each platform has its own format. Usually, they are provided by the user in the web app UI, using the CLI or by calling the npm package.

For each file, a [`FileInput` interface](/pipeline/parse/File.ts) has to be created, which along with metadata, contains the `slice(start?: number, end?: number): ArrayBuffer` function that must return a slice of the file in the specified range. This function is environment dependent. Since files may be large (several GBs) we don't read the entire file content into memory, instead we allow parsers to stream them. 

## 2. Parsing

This step takes the input files and parses them into [P-interfaces](/pipeline/parse/Types.ts) (PGuild, PChannel, PAuthor, PMessage); it is a common format for all platforms, so it is easy to work with different and potentially new platforms.
Each platform creates a class that extends the [`Parser` class](/pipeline/parse/Parser.ts) and implements the `parse(file: FileInput)` function. Check [existing implementations](/pipeline/parse/parsers/) for reference.

If you want to support a new platform, you can find some guidance in [PARSER.md](./PARSER.md) document.

## 3. Processing & Analysis

This part is about generating the [`Database` object](/pipeline/process/Types.ts) from the P-interfaces generated by the parsers. The class responsible for this is [`DatabaseBuilder`](/pipeline/process/DatabaseBuilder.ts).

During processing, incoming objects from the parser can be:
* `PGuild`, `PChannel` and `PAuthor`: they are added to an [IndexedMap](/pipeline/process/IndexedMap.ts) and an index is assigned to them if their ID hasn't been seen before. Subsequent processing will query the index by ID to only store the index.
* `PMessage`: each message is added to a processing queue in its corresponding [`ChannelMessages`](/pipeline/process/ChannelMessages.ts) (each channel has its own). This class handles duplicate messages and overlapping in input files. At some point, the queue will be split into groups of messages that were sent by the same author (PMessageGroup) and passed to the [`MessageProcessor`](/pipeline/process/MessageProcessor.ts) for processing that will have to return a [`Message`](/pipeline/process/Types.ts) array for each group.  
The `MessageProcessor` class handles tokenization, language detection and sentiment computation. It inserts the data back into the data stores in the `DatabaseBuilder` instance (words, emojis, mentions, etc.).

After all input files have been processed, it sorts some data stores (IndexedMaps) based on the number of messages. Since this changes all indexes, we have to remap the old indexes to the new ones. Also, it filters out some data we don't want to keep. Unfortunately, it also means deserializing and serializing all messages again.

I hope the code is clear enough to understand how it works.

## 4. Compression and Encoding

After the `Database` object is ready, we store it in the standalone report HTML file. We want it to be as small as possible and can't be in binary since HTML is a text format. So we compress it with [`fflate`](https://www.npmjs.com/package/fflate) and then encode it with [`base91`](/pipeline/compression/Base91.ts) using HTML-friendly chars. The result is a string that is stored in the HTML file.

When the report HTML is loaded (via blob or file://) we do the inverse process to get the `Database` object: decode the string with `base91` and decompress it with `fflate`.

## 5. Aggregate / Blocks

[TODO WRITE]

## About message serialization

Instead of storing messages as JS Objects, they are serialized into a custom binary format. This is done to reduce memory consumption and improve performance. It also allows us to store many more messages at once and thus, reports are smaller.

Overview of serialization files:

- [MessageSerialization.ts](/pipeline/serialization/MessageSerialization.ts): contains `writeMessage` and `readMessage` to serialize and deserialize a single message. Make heavy use of `writeIndexCounts` and `readIndexCounts` to serialize and deserialize IndexCounts (see below).
- [IndexCountsSerialization.ts](/pipeline/serialization/IndexCountsSerialization.ts): allows serialization of [IndexCounts](/pipeline/process/IndexCounts.ts) which are pairs of `[index, count]`. We use this format to point out which and how many of each object are used in a given context (e.g. emojis in a message)
- [MessagesArray.ts](/pipeline/serialization/MessagesArray.ts): useful abstraction to work with serialized messages as if they were a regular Array (push and iteration)
- [MessageView.ts](/pipeline/serialization/MessageView.ts): class implementing the `Message` interface that deserializes data on demand. Useful if you don't need all the data at once. Used in aggregation.

